library(twitteR)
twitter_key <- "JooeFz2g7i4VbWAIwpddOL4dr"
twitter_secret <- "d4FP2t9O8Nxa8lAcoNu8x3YtQpJWE41dJ0B6aeVhQnazEt0j44"
access_token <- "463023825-aTRXAw2V3WYXjWhMCOpOHEWrTe2yVOGGvdwyI0E8"
access_secret <- "Yy5FnjsYD34jCablVxY1AviescYo1ryD843lJrTGytP4t"
oauth <- setup_twitter_oauth(twitter_key,
twitter_secret,
access_token,
access_secret)
require(twitteR)
search.string <- "#albertrivera"
no.of.tweets <- 1000
myTweets <- searchTwitter(search.string, n=no.of.tweets,
since='2018-06-22', until='2018-06-28', lang="en")
head(myTweets)
save(myTweets, file="myTweets.Rda")
## ----regexClean----------------------------------------------------------
require(twitteR)
showTweets <- TRUE
if (!exists("myTweets")) load("myTweets.Rda")
tweets.text <- sapply(myTweets, function(x) x$getText())
# Replace @UserName
tweets.text <- gsub("@\\w+", "", tweets.text)
# Remove punctuation
tweets.text <- gsub("[[:punct:]]", "", tweets.text)
# Remove links
tweets.text <- gsub("http\\w+", "", tweets.text)
# Remove tabs
tweets.text <- gsub("[ |\t]{2,}", "", tweets.text)
# remove codes that are neither characters nor digits
tweets.text<-  gsub( '[^A-z0-9_]', ' ', tweets.text)
# Set characters to lowercase
tweets.text <- tolower(tweets.text)
# Replace blank space
tweets.text <- gsub("rt", "", tweets.text)
# Remove blank spaces at the beginning
tweets.text <- gsub("^ ", "", tweets.text)
# Remove blank spaces at the end
tweets.text <- gsub(" $", "", tweets.text)
head(tweets.text)
## ------------------------------------------------------------------------
require("tm")
#create corpus
tweets.text.corpus <- Corpus(VectorSource(tweets.text))
head(stopwords())
#clean up by removing stop words
tweets.text.corpus <- tm_map(tweets.text.corpus, function(x) removeWords(x, stopwords()))
head(tweets.text.corpus)
## ----message=FALSE-------------------------------------------------------
#generate wordcloud
require(wordcloud)
wordcloud(tweets.text.corpus, min.freq = 2, scale=c(7,0.5),
colors=brewer.pal(8, "Dark2"),random.color= TRUE, random.order = FALSE,
max.words = 150)
require(twitteR)
search.string <- "#rivera"
no.of.tweets <- 1000
myTweets <- searchTwitter(search.string, n=no.of.tweets,
since='2018-06-22', until='2018-06-28', lang="en")
head(myTweets)
save(myTweets, file="myTweets.Rda")
## ----regexClean----------------------------------------------------------
require(twitteR)
showTweets <- TRUE
if (!exists("myTweets")) load("myTweets.Rda")
tweets.text <- sapply(myTweets, function(x) x$getText())
# Replace @UserName
tweets.text <- gsub("@\\w+", "", tweets.text)
# Remove punctuation
tweets.text <- gsub("[[:punct:]]", "", tweets.text)
# Remove links
tweets.text <- gsub("http\\w+", "", tweets.text)
# Remove tabs
tweets.text <- gsub("[ |\t]{2,}", "", tweets.text)
# remove codes that are neither characters nor digits
tweets.text<-  gsub( '[^A-z0-9_]', ' ', tweets.text)
# Set characters to lowercase
tweets.text <- tolower(tweets.text)
# Replace blank space
tweets.text <- gsub("rt", "", tweets.text)
# Remove blank spaces at the beginning
tweets.text <- gsub("^ ", "", tweets.text)
# Remove blank spaces at the end
tweets.text <- gsub(" $", "", tweets.text)
head(tweets.text)
## ------------------------------------------------------------------------
require("tm")
#create corpus
tweets.text.corpus <- Corpus(VectorSource(tweets.text))
head(stopwords())
#clean up by removing stop words
tweets.text.corpus <- tm_map(tweets.text.corpus, function(x) removeWords(x, stopwords()))
head(tweets.text.corpus)
## ----message=FALSE-------------------------------------------------------
#generate wordcloud
require(wordcloud)
wordcloud(tweets.text.corpus, min.freq = 2, scale=c(7,0.5),
colors=brewer.pal(8, "Dark2"),random.color= TRUE, random.order = FALSE,
max.words = 150)
require(twitteR)
search.string <- "#rajoy"
no.of.tweets <- 1000
myTweets <- searchTwitter(search.string, n=no.of.tweets,
since='2018-06-22', until='2018-06-28', lang="en")
head(myTweets)
save(myTweets, file="myTweets.Rda")
## ----regexClean----------------------------------------------------------
require(twitteR)
showTweets <- TRUE
if (!exists("myTweets")) load("myTweets.Rda")
tweets.text <- sapply(myTweets, function(x) x$getText())
# Replace @UserName
tweets.text <- gsub("@\\w+", "", tweets.text)
# Remove punctuation
tweets.text <- gsub("[[:punct:]]", "", tweets.text)
# Remove links
tweets.text <- gsub("http\\w+", "", tweets.text)
# Remove tabs
tweets.text <- gsub("[ |\t]{2,}", "", tweets.text)
# remove codes that are neither characters nor digits
tweets.text<-  gsub( '[^A-z0-9_]', ' ', tweets.text)
# Set characters to lowercase
tweets.text <- tolower(tweets.text)
# Replace blank space
tweets.text <- gsub("rt", "", tweets.text)
# Remove blank spaces at the beginning
tweets.text <- gsub("^ ", "", tweets.text)
# Remove blank spaces at the end
tweets.text <- gsub(" $", "", tweets.text)
head(tweets.text)
## ------------------------------------------------------------------------
require("tm")
#create corpus
tweets.text.corpus <- Corpus(VectorSource(tweets.text))
head(stopwords())
#clean up by removing stop words
tweets.text.corpus <- tm_map(tweets.text.corpus, function(x) removeWords(x, stopwords()))
head(tweets.text.corpus)
## ----message=FALSE-------------------------------------------------------
#generate wordcloud
require(wordcloud)
wordcloud(tweets.text.corpus, min.freq = 2, scale=c(7,0.5),
colors=brewer.pal(8, "Dark2"),random.color= TRUE, random.order = FALSE,
max.words = 150)
require(twitteR)
search.string <- "salvini"
no.of.tweets <- 1000
myTweets <- searchTwitter(search.string, n=no.of.tweets,
since='2018-06-22', until='2018-06-28', lang="en")
head(myTweets)
save(myTweets, file="myTweets.Rda")
## ----regexClean----------------------------------------------------------
require(twitteR)
showTweets <- TRUE
if (!exists("myTweets")) load("myTweets.Rda")
tweets.text <- sapply(myTweets, function(x) x$getText())
# Replace @UserName
tweets.text <- gsub("@\\w+", "", tweets.text)
# Remove punctuation
tweets.text <- gsub("[[:punct:]]", "", tweets.text)
# Remove links
tweets.text <- gsub("http\\w+", "", tweets.text)
# Remove tabs
tweets.text <- gsub("[ |\t]{2,}", "", tweets.text)
# remove codes that are neither characters nor digits
tweets.text<-  gsub( '[^A-z0-9_]', ' ', tweets.text)
# Set characters to lowercase
tweets.text <- tolower(tweets.text)
# Replace blank space
tweets.text <- gsub("rt", "", tweets.text)
# Remove blank spaces at the beginning
tweets.text <- gsub("^ ", "", tweets.text)
# Remove blank spaces at the end
tweets.text <- gsub(" $", "", tweets.text)
head(tweets.text)
## ------------------------------------------------------------------------
require("tm")
#create corpus
tweets.text.corpus <- Corpus(VectorSource(tweets.text))
head(stopwords())
#clean up by removing stop words
tweets.text.corpus <- tm_map(tweets.text.corpus, function(x) removeWords(x, stopwords()))
head(tweets.text.corpus)
## ----message=FALSE-------------------------------------------------------
#generate wordcloud
require(wordcloud)
wordcloud(tweets.text.corpus, min.freq = 2, scale=c(7,0.5),
colors=brewer.pal(8, "Dark2"),random.color= TRUE, random.order = FALSE,
max.words = 150)
require(twitteR)
search.string <- "puigdemont"
no.of.tweets <- 1000
myTweets <- searchTwitter(search.string, n=no.of.tweets,
since='2018-06-22', until='2018-06-28', lang="en")
head(myTweets)
save(myTweets, file="myTweets.Rda")
## ----regexClean----------------------------------------------------------
require(twitteR)
showTweets <- TRUE
if (!exists("myTweets")) load("myTweets.Rda")
tweets.text <- sapply(myTweets, function(x) x$getText())
# Replace @UserName
tweets.text <- gsub("@\\w+", "", tweets.text)
# Remove punctuation
tweets.text <- gsub("[[:punct:]]", "", tweets.text)
# Remove links
tweets.text <- gsub("http\\w+", "", tweets.text)
# Remove tabs
tweets.text <- gsub("[ |\t]{2,}", "", tweets.text)
# remove codes that are neither characters nor digits
tweets.text<-  gsub( '[^A-z0-9_]', ' ', tweets.text)
# Set characters to lowercase
tweets.text <- tolower(tweets.text)
# Replace blank space
tweets.text <- gsub("rt", "", tweets.text)
# Remove blank spaces at the beginning
tweets.text <- gsub("^ ", "", tweets.text)
# Remove blank spaces at the end
tweets.text <- gsub(" $", "", tweets.text)
head(tweets.text)
## ------------------------------------------------------------------------
require("tm")
#create corpus
tweets.text.corpus <- Corpus(VectorSource(tweets.text))
head(stopwords())
#clean up by removing stop words
tweets.text.corpus <- tm_map(tweets.text.corpus, function(x) removeWords(x, stopwords()))
head(tweets.text.corpus)
## ----message=FALSE-------------------------------------------------------
#generate wordcloud
require(wordcloud)
wordcloud(tweets.text.corpus, min.freq = 2, scale=c(7,0.5),
colors=brewer.pal(8, "Dark2"),random.color= TRUE, random.order = FALSE,
max.words = 150)
require(twitteR)
search.string <- "albert rivera"
no.of.tweets <- 1000
myTweets <- searchTwitter(search.string, n=no.of.tweets,
since='2018-06-22', until='2018-06-28', lang="en")
head(myTweets)
save(myTweets, file="myTweets.Rda")
## ----regexClean----------------------------------------------------------
require(twitteR)
showTweets <- TRUE
if (!exists("myTweets")) load("myTweets.Rda")
tweets.text <- sapply(myTweets, function(x) x$getText())
# Replace @UserName
tweets.text <- gsub("@\\w+", "", tweets.text)
# Remove punctuation
tweets.text <- gsub("[[:punct:]]", "", tweets.text)
# Remove links
tweets.text <- gsub("http\\w+", "", tweets.text)
# Remove tabs
tweets.text <- gsub("[ |\t]{2,}", "", tweets.text)
# remove codes that are neither characters nor digits
tweets.text<-  gsub( '[^A-z0-9_]', ' ', tweets.text)
# Set characters to lowercase
tweets.text <- tolower(tweets.text)
# Replace blank space
tweets.text <- gsub("rt", "", tweets.text)
# Remove blank spaces at the beginning
tweets.text <- gsub("^ ", "", tweets.text)
# Remove blank spaces at the end
tweets.text <- gsub(" $", "", tweets.text)
head(tweets.text)
## ------------------------------------------------------------------------
require("tm")
#create corpus
tweets.text.corpus <- Corpus(VectorSource(tweets.text))
head(stopwords())
#clean up by removing stop words
tweets.text.corpus <- tm_map(tweets.text.corpus, function(x) removeWords(x, stopwords()))
head(tweets.text.corpus)
## ----message=FALSE-------------------------------------------------------
#generate wordcloud
require(wordcloud)
wordcloud(tweets.text.corpus, min.freq = 2, scale=c(7,0.5),
colors=brewer.pal(8, "Dark2"),random.color= TRUE, random.order = FALSE,
max.words = 150)
img_list <- read_html("https://www.imdb.com/list/ls070150896/")
library(rvest)
img_list <- read_html("https://stats.nba.com/player/2544/shots-dash/")
url <- "https://stats.nba.com/player/2544/shots-dash/"
imgsrc <- read_html(url) %>%
html_node(xpath = '//*/img') %>%
html_attr('src')
imgsrc
url <- "https://stats.nba.com/player/2544/shots-dash/"
imgsrc <- read_html(url) %>%
html_node(xpath = 'player-img') %>%
html_attr('src')
imgsrc
url <- "https://stats.nba.com/player/2544/shots-dash/"
imgsrc <- read_html(url) %>%
html_node(xpath = '. player-img') %>%
html_attr('src')
imgsrc
url <- "https://stats.nba.com/player/2544/shots-dash/"
imgsrc <- read_html(url) %>%
html_node('. player-img') %>%
html_attr('src')
imgsrc
url <- "https://stats.nba.com/player/2544/shots-dash/"
imgsrc <- read_html(url) %>%
html_node('. player-img') %>%
html_attr('script')
imgsrc
url <- "https://stats.nba.com/player/2544/shots-dash/"
imgsrc <- read_html(url) %>%
html_node("script")
imgsrc
rm(list = ls())
players_url = "http://stats.nba.com/stats/commonallplayers?LeagueID=00&Season=2015-16&IsOnlyCurrentSeason=0"
request_headers = c(
"accept-encoding" = "gzip, deflate, sdch",
"accept-language" = "en-US,en;q=0.8",
"cache-control" = "no-cache",
"connection" = "keep-alive",
"host" = "stats.nba.com",
"pragma" = "no-cache",
"upgrade-insecure-requests" = "1",
"user-agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9"
)
request = GET(players_url, add_headers(request_headers))
required_packages = c(
"shiny",
"ggplot2",
"hexbin",
"dplyr",
"httr",
"jsonlite"
)
packages_to_install = required_packages[!(required_packages %in% installed.packages()[, 1])]
if (length(packages_to_install) > 0) {
install.packages(packages_to_install, repos = "https://cran.rstudio.com")
}
request_headers = c(
"accept-encoding" = "gzip, deflate, sdch",
"accept-language" = "en-US,en;q=0.8",
"cache-control" = "no-cache",
"connection" = "keep-alive",
"host" = "stats.nba.com",
"pragma" = "no-cache",
"upgrade-insecure-requests" = "1",
"user-agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9"
)
request = GET(players_url, add_headers(request_headers))
library(httr)
library(dplyr)
library(jsonlite)
request = GET(players_url, add_headers(request_headers))
players_data = fromJSON(content(request, as = "text"))
players = tbl_df(data.frame(players_data$resultSets$rowSet[[1]], stringsAsFactors = FALSE))
names(players) = tolower(players_data$resultSets$headers[[1]])
players = mutate(players,
person_id = as.numeric(person_id),
rosterstatus = as.logical(as.numeric(rosterstatus)),
from_year = as.numeric(from_year),
to_year = as.numeric(to_year),
team_id = as.numeric(team_id)
)
if (Sys.Date() <= as.Date("2017-10-20")) {
players = mutate(players, to_year = pmin(to_year, 2016))
}
players$name = sapply(players$display_last_comma_first, function(s) {
paste(rev(strsplit(s, ", ")[[1]]), collapse = " ")
})
first_year_of_data = 1996
last_year_of_data = max(players$to_year)
season_strings = paste(first_year_of_data:last_year_of_data,
substr(first_year_of_data:last_year_of_data + 1, 3, 4),
sep = "-")
names(season_strings) = first_year_of_data:last_year_of_data
available_players = filter(players, to_year >= first_year_of_data)
names_table = table(available_players$name)
dupe_names = names(names_table[which(names_table > 1)])
available_players$name[available_players$name %in% dupe_names] = paste(
available_players$name[available_players$name %in% dupe_names],
available_players$person_id[available_players$name %in% dupe_names]
)
available_players$lower_name = tolower(available_players$name)
available_players = arrange(available_players, lower_name)
find_player_by_name = function(n) {
filter(available_players, lower_name == tolower(n))
}
find_player_id_by_name = function(n) {
find_player_by_name(n)$person_id
}
default_player = find_player_by_name("Stephen Curry")
default_years = as.character(default_player$from_year:default_player$to_year)
default_seasons = as.character(season_strings[default_years])
default_season = rev(default_seasons)[1]
default_season_type = "Regular Season"
player_photo_url = function(player_id) {
paste0("http://stats.nba.com/media/players/230x185/", player_id, ".png")
}
str(players)
View(players)
View(players)
View(default_player)
View(default_player)
rm(list = ls())
library(rjson)
players_url = "http://stats.nba.com/stats/commonallplayers?
LeagueID=00&Season=2015-16&IsOnlyCurrentSeason=0"
request_headers = c(
"accept-encoding" = "gzip, deflate, sdch",
"accept-language" = "en-US,en;q=0.8",
"cache-control" = "no-cache",
"connection" = "keep-alive",
"host" = "stats.nba.com",
"pragma" = "no-cache",
"upgrade-insecure-requests" = "1",
"user-agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9"
)
request = GET(players_url, add_headers(request_headers))
players_url = "http://stats.nba.com/stats/commonallplayers?LeagueID=00&Season=2015-16&IsOnlyCurrentSeason=0"
request = GET(players_url, add_headers(request_headers))
players_data = fromJSON(content(request, as = "text"))
players = tbl_df(data.frame(players_data$resultSets$rowSet[[1]], stringsAsFactors = FALSE))
names(players) = tolower(players_data$resultSets$headers[[1]])
players = mutate(players,
person_id = as.numeric(person_id),
rosterstatus = as.logical(as.numeric(rosterstatus)),
from_year = as.numeric(from_year),
to_year = as.numeric(to_year),
team_id = as.numeric(team_id)
)
request = GET(players_url, add_headers(request_headers))
rm(list = ls(9))
rm(list = ls())
players_url = "http://stats.nba.com/stats/commonallplayers?LeagueID=00&Season=2015-16&IsOnlyCurrentSeason=0"
request_headers = c(
"accept-encoding" = "gzip, deflate, sdch",
"accept-language" = "en-US,en;q=0.8",
"cache-control" = "no-cache",
"connection" = "keep-alive",
"host" = "stats.nba.com",
"pragma" = "no-cache",
"upgrade-insecure-requests" = "1",
"user-agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9"
)
request = GET(players_url, add_headers(request_headers))
players_data = fromJSON(content(request, as = "text"))
players = tbl_df(data.frame(players_data$resultSets$rowSet[[1]], stringsAsFactors = FALSE))
library("jsonlite", lib.loc="~/R/R-3.4.2/library")
players_url = "http://stats.nba.com/stats/commonallplayers?LeagueID=00&Season=2015-16&IsOnlyCurrentSeason=0"
request_headers = c(
"accept-encoding" = "gzip, deflate, sdch",
"accept-language" = "en-US,en;q=0.8",
"cache-control" = "no-cache",
"connection" = "keep-alive",
"host" = "stats.nba.com",
"pragma" = "no-cache",
"upgrade-insecure-requests" = "1",
"user-agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9"
)
request = GET(players_url, add_headers(request_headers))
players_data = fromJSON(content(request, as = "text"))
players = tbl_df(data.frame(players_data$resultSets$rowSet[[1]], stringsAsFactors = FALSE))
names(players) = tolower(players_data$resultSets$headers[[1]])
players = mutate(players,
person_id = as.numeric(person_id),
rosterstatus = as.logical(as.numeric(rosterstatus)),
from_year = as.numeric(from_year),
to_year = as.numeric(to_year),
team_id = as.numeric(team_id)
)
url <- movies_list %>%
html_nodes(" h3 a") %>%
html_attr('href') %>%
na.exclude()
library(rvest)
url <- movies_list %>%
html_nodes(" h3 a") %>%
html_attr('href') %>%
na.exclude()
movies_list <- read_html("https://www.imdb.com/list/ls070150896/")
url <- movies_list %>%
html_nodes(" h3 a") %>%
html_attr('href') %>%
na.exclude()
url <- grep("/title/", url, value = TRUE)
url
#assign main path to every movie
for (i in 1:length(url)){
films <- paste0("http://www.imdb.com", url, sep = "")
}
star_wars <- list()
for ( i in 1:length(films)){
star_wars[[i]] <- read_html(films[i])
}
cast_star <- list()
for(i in 1:length(films)){
cast_star[[i]] <- star_wars[[i]] %>%
html_nodes("#titleCast .itemprop span") %>%
html_text()
cast_star
}
cast_star
rm(list = ls())
